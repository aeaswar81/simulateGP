---
title: "Simulating GWAS summary data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Simulating GWAS summary data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(simulateGP)
```


GWAS summary data can be generated in two ways

1. Given a set of parameters, generate the genotypes for a set of individuals and the phenotypes that arise from those genotypes. Then obtain linear regression estimates from these simulated individual level data
2. Given a set of parameters, simulate the summary level data directly

here we look at (1). 

Given the following inputs:

1. True causal effects of a set of SNPs
2. Minor allele frequency of those SNPs
3. Heritability explained by those SNPs
4. Sample size in which they are estimated

We would like to obtain:

1. Standard error of SNP effect on trait
2. Sampled estimate of SNP effect on trait
3. P-value of SNP effect estimate for trait

Strategy is to begin by obtaining the expected standard error for the true causal effect for each SNP, and then sampling an estimated SNP effect based on the expected standard error.

## Expected standard error

For a linear model

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i
$$

The coefficient estimates are

$$
\hat{\beta}_1 = cov(x,y) / var(x)
$$

and

$$
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}
$$

The standard error of a regression coefficient is:

$$
s_{\hat{\beta}_1} = \sqrt{\frac{\sum{\epsilon_i^2}}{(n-2) \sum{(x_i - \bar{x})^2}}}
$$

This can be broken down. The denominator is the sum of squares of X:

$$
\begin{aligned}
(x_i - \bar{x})^2 &= (n-1) Var(x) \\
&\approx (n-1) 2p(1-p)
\end{aligned}
$$

where $p$ is the allele frequency of the SNP and $n$ is the sample size. The numerator is the mean squared error, which relates to the variance unexplained in Y:

$$
\begin{aligned}
\sum \epsilon_i^2 &= \sum(y_i - \hat{y}_i)^2 / (n-1) \\
&= Var(y) - r^2 Var(y) \\
&\approx Var(y) - \hat{\beta}_1^2 Var(x) \\
&\approx Var(y) - 2 p(1-p) \hat{\beta}_1^2
\end{aligned}
$$

Quick check:

```{r}
n <- 100
y <- rnorm(n)
x <- rnorm(n)

summary(lm(y ~ x))$coefficients[2,2]
sqrt((var(y) - cor(x,y)^2 * var(y)) / (var(x) * (n-2)))
```

## Sampling the effect estimate

Given a standard error $s_{\hat{\beta}_1}$, we can obtain an estimated value for $\beta_1$ using

$$
\hat{\beta}_1 \sim N(\beta_1, s_{\hat{\beta}_1})
$$

Question: Do I need to update the expected standard error based on the sampled value of $\hat{\beta}_1$?


## Sampling a set of effect estimates

Suppose we need a set of SNPs to explain some proportion of the variance in a trait.

When specifying the SNP effects there is a simple model that can relate to a SNP to its contribution to heritability

$$
h^2_j = \frac{2p_j(1-p_j)\beta^2_j}{Var(y)}
$$

Also

$$
\begin{aligned}
h_j^2 &= \frac{Cov(x_j, y)^2}{Var(x_j)Var(y)} \\
&= \frac{\beta_j^2}{Var(y)}
\end{aligned}
$$

and

$$
h^2 = \sum^M_j h_j^2
$$

If each SNP has MAF of $p_j$ and effect of $\beta_j$, then

$$
V_G = \sum^M_j 2p_j(1-p_j)\beta^2_j
$$

$$
V_E = \frac{V_G(1 - h^2)}{h^2}
$$
and

$$
Var(y) = V_G + V_E
$$

Once SNP effects have been sampled then they can be scaled to represent a phenotype with variance of 1:

$$
\beta_j^* = \frac{\beta_j}{\sqrt{Var(y)}}
$$

Relating SNP effects to allele frequencies can be done by specifying a selection model. Generalise the relationship between the SNP effects, allele frequencies and selection coefficient of the trait:

$$
\beta_j \sim N(0, [2p_j (1-p_j)]^S \sigma^2_\beta)
$$

Where S is the parameter describing the selection acting on the trait. Here $\sigma^2_\beta = V_G / M$ if all SNPs were scaled to have variance of 1 and mean of 0.

$$
\begin{aligned}
\sigma^2_\beta &= \sum^M_j (\beta_j-\bar{\beta})^2 \\
&= \sum^M_j \beta_j^2
\end{aligned}
$$

when the distribution of SNP effects is centred. This can be adapted to the BayesS model, in which some proportion of SNPs have no effect:

$$
\beta_j \sim N(0, [2p_j (1-p_j)]^S \sigma^2_\beta)\pi + \vartheta (1 - \pi)
$$

I think ultimately the presence of $\sigma^2_\beta$ in this model does not actually mean anything, it scales the SNP effects with respect to the variance of the phenotype. Use the following function to generate SNP effects for a set of SNPs according to specified MAF, heritability and selection model:

```{r}
generate_gwas_params
```

Negative selection:

```{r}
g <- generate_gwas_params(maf=runif(1000, 0.01, 0.5), h2=0.4, S=-2)
plot(g)
```

Positive selection:

```{r}
g <- generate_gwas_params(maf=runif(1000, 0.01, 0.5), h2=0.4, S=2)
plot(g)
```

Neutral model:

```{r}
g <- generate_gwas_params(maf=runif(1000, 0.01, 0.5), h2=0.4, S=0)
plot(g)
```

Note that the genetic variance is the heritability

```{r}
sum(g$beta^2 * 2 * g$maf * (1-g$maf))
```


Bringing it all together, we can simulate a set of GWAS summary stats by first specifying the true effects we want, and then obtaining sampled effects, standard errors and p-values.

```{r}
# Simulate 1 million SNPs, with 10 large effects, a polygenic background of 10000 SNPs, and all others with no effects
param <- rbind(
	generate_gwas_params(maf=runif(10, 0.4, 0.5), h2=0.1, S=0),
	generate_gwas_params(maf=runif(10000, 0.01, 0.5), h2=0.3, S=0),
	generate_gwas_params(maf=runif(1000000-10-10000, 0.01, 0.5), h2=0, S=0)
)

# Generate GWAS summary stats
res <- generate_gwas_ss(param$beta, param$maf, 450000)
```

Compare to individual level data results to summary data simulations


```{r}
set.seed(100)
h2 <- 0.1
nid <- 10000
param <- generate_gwas_params(maf=runif(100, 0.01, 0.5), h2=h2, S=0)
res1 <- generate_gwas_ss(param$beta, param$maf, nid)

g <- lapply(param$maf, function(x) rbinom(nid, 2, x)) %>%
	bind_cols %>%
	as.matrix

score <- g %*% param$beta
err <- rnorm(nid, mean=0, sd=sqrt(var(score) * (1-h2) / h2))
y <- score + err
res2 <- gwas(y, g)
plot(res2$bhat ~ res1$bhat)
plot(res2$se, res1$se)
```


**TODO: simulate effect estimates for SNPs given LD matrix**



## Weak instrument bias


https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf

https://en.wikipedia.org/wiki/Instrumental_variables_estimation#Interpretation_as_two-stage_least_squares

https://www.youtube.com/watch?v=JK-8XNIoAkI

http://econ.lse.ac.uk/staff/spischke/ec533/Weak%20IV.pdf

https://ocw.mit.edu/courses/economics/14-384-time-series-analysis-fall-2013/lecture-notes/MIT14_384F13_lec7and8.pdf

https://www.nber.org/econometrics_minicourse_2018/2018si_methods.pdf

https://www.ncbi.nlm.nih.gov/pubmed/21414999

https://stats.stackexchange.com/questions/48366/standard-errors-for-covariance-estimate-in-r



Parameters include:

- Sample size
- Number of SNPs
- Allele frequencies of SNPs
- Heritability of the phenotype
- Effects of each SNP on the phenotype






```{r}

h2 <- 0.4
nsnp <- 100
nid <- 10000

G <- make_geno(nid, nsnp, 0.5)
Gs <- scale(G)


bsim <- rnorm(nsnp, 0, sqrt(h2))

bv <- Gs %*% bsim


var(bv)
sd(bv)


# h2 <- b^2 * 2 * maf * (1 - maf) / vy
# ve <- vy - vg
```



```{r}
h2 <- 0.4
nsnp <- 100
nid <- 10000

G <- make_geno(nid, nsnp, 0.5)
b <- choose_effects(nsnp, h2)
y <- make_phen(b, G)
head(y)
var(y)

bv <- G %*% b
cor(bv, y)^2
bhat <- gwas(y, G)

```


```{r}
btheory <- theoretical_gwas(b, rep(0.5, nsnp), h2, nid)
plot(btheory$beta ~ bhat$bhat)
plot(btheory$se ~ bhat$se)
plot(log10(btheory$pval) ~ log10(bhat$pval))
```

Standard error of beta

$$
s_{\hat{\beta}} = \sqrt{\frac{MSE}{SSX}}
$$


Check that SSX is calculated correctly when using MAF to obtain var(X)

```{r}
set.seed(100)
param <- expand.grid(
	af = runif(1000)/2
)
param$n <- sample(300:10000, 1000)

for(i in 1:nrow(param))
{
	g <- rbinom(param$n[i], 2, param$af[i])
	param$ssx_emp[i] <- sum((g - mean(g))^2)
	param$ssx_emp2[i] <- var(g) * (param$n[i] - 1)
	param$ssx_exp[i] <- 2 * param$af[i] * (1 - param$af[i]) * (param$n[i] - 1)
}

plot(param$ssx_emp, param$ssx_emp2)
plot(param$ssx_emp, param$ssx_exp)

```

Now check MSE


```{r}
param$b <- rnorm(nrow(param))
```

```{r}
for(i in 1:nrow(param))
{
	g <- rbinom(param$n[i], 2, param$af[i])
	y <- g * param$b[i] + rnorm(param$n[i])
	param$vy[i] <- var(y)
	param$r2[i] <- cor(g,y)^2
	mod <- lm(y ~ g)
	param$mse[i] <- anova(mod)[[3]][2]
	param$mse_exp[i] <- expected_mse(param$b[i], param$af[i], param$vy[i])
	mod2 <- summary(mod)$coefficients
	param$bhat[i] <- mod2[2,1]
	param$se[i] <- mod2[2,2]
	param$se_theor[i] <- expected_se(param$b[i], param$af[i], param$n[i], param$vy[i])
}
```

```{r}
plot(param$mse, param$mse_exp)
plot(param$bhat, param$b)
plot(param$se, param$se_theor)
hist(param$r2)
ggplot(param, aes(mse, mse_exp)) +
geom_point(aes(colour=af))

ggplot(param, aes(mse, mse_exp)) +
geom_point(aes(colour=r2))

```


Weak instrument bias

```{r}
library(mvtnorm)

get_biv_bias <- function(n, rho, beta, pi1)
{
	err <- rmvnorm(n, c(0,0), matrix(c(1,rho,rho,1), 2, 2))
	eta <- err[,1]
	zeta <- err[,2]
	z <- rnorm(n)
	x <- pi1 * z + zeta
	y <- beta * x + eta

	Pz <- z %*% solve(t(z) %*% z) %*% t(z)

	biv <- solve(t(x) %*% Pz %*% x) %*% t(x) %*% Pz %*% y
	bias <- solve(t(x) %*% Pz %*% x) %*% t(x) %*% Pz %*% eta

	return(c(biv, bias))
}


param <- expand.grid(
	n = c(100),
	pi1=seq(0, 0.1, by=0.02),
	beta=c(0,1),
	rho=seq(0, 0.8, by=0.2),
	sim=1:200
)

dim(param)
for(i in 1:nrow(param))
{
	mod <- get_biv_bias(param$n[i], param$rho[i], param$beta[i], param$pi1[i])
	param$biv[i] <- mod[1]
	param$bias[i] <- mod[2]
}

library(dplyr)
params <- group_by(param, n, pi1, beta, rho) %>%
summarise(biv=median(biv), bias=median(bias), nsim=n())

ggplot(params, aes(y=bias, x=pi1, group=as.factor(rho))) +
geom_point(aes(colour=as.factor(rho))) +
geom_line(aes(colour=as.factor(rho))) +
facet_grid(n ~ beta)
```


```{r}

n <- 1000
pi1 <- 1
beta <- 1
err <- rmvnorm(n, c(0,0), matrix(c(1,0.8,0.8,1), 2, 2))
eta <- err[,1]
zeta <- err[,2]
z <- rnorm(n)
x <- pi1 * z + zeta
y <- beta * x + eta

Pz <- z %*% solve(t(z) %*% z) %*% t(z)

biv <- solve(t(x) %*% Pz %*% x) %*% t(x) %*% Pz %*% y
bias <- solve(t(x) %*% Pz %*% x) %*% t(x) %*% Pz %*% eta

t(x) %*% x
t(x) %*% Pz %*% x

cor(x, z)^2
t(x) %*% x * cor(x,z)^2

var(x) * (length(x)-1)

sum((x - mean(x))^2) / (n-1) * (n)

sum(x^2)

t(x) %*% x / t(x) %*% Pz %*% x

sum(z^2) / (length(z) - 1)


cov(x,y) / var(x)

solve(t(x) %*% x) %*% t(x) %*% y




sum(x^2)



u <- rnorm(n)
z <- rnorm(n)
x <- u * 2 + z + rnorm(n)
y <- u * -2 + x * 2 + rnorm(n)

res1 <- residuals(lm(x ~ z))
res2 <- residuals(lm(y ~ x))

cor(res1, res2)

cor(u, x)^2 * cor(y, u)^2


```

```
(x'x)-1 x'(xb + e)
(x'x)-1 x'xb + x'e
```

```{r}
solve(t(x) %*% x) %*% t(x) %*% (x * 5 + eta)

```

```
cov(g,x) = cov(g, b1*g + a1*u + e)
= b1 var(g) + a1 cov(g, u)
```

```
cov(g,y) = cov(g, b2*x + a2*u + e)
= b2 b1 var(g) + b2 a1 cov(g, u) + a2 cov(g, u)
= b2 b1 var(g) + cov(g,u) (b2 a1 + a2)
= b2 (b1 var(g) + a1 cov(g, u)) + a2 cov(g, u)
```


```{r}
a <- rnorm(1000)
b <- rnorm(1000)
cov(a,b)
sum((a-mean(a)) * (b - mean(b)))

sum(a * b) + sum((a-mean(a))^2) * sum((b-mean(b))^2)
```



Specify

- Number of SNPs


```{r}
fun <- function(nsnp, pi0, s)
{

}
```









